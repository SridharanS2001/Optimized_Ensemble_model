# -*- coding: utf-8 -*-
"""Models_Hyperparameter_tuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1aLTwcUHS5-7B5JOPytv19dd2bIxG_eWw
"""

pip install scikit-optimize

pip install optuna

"""### **For Mm prediction**"""

import numpy as np
import pandas as pd
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from skopt import BayesSearchCV

data = pd.read_csv('/content/Processed_RawData_v2d (2).csv')
X = data.iloc[:, 1:7]
y = data.iloc[:, 17]

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

models = {
    "SVR": (SVR(), {
        "C": (1e-3, 1e3, "log-uniform"),
        "epsilon": (1e-3, 1.0, "log-uniform"),
        "gamma": (1e-4, 1.0, "log-uniform"),
        "kernel": ["linear", "rbf"]
    }),
    "RandomForest": (RandomForestRegressor(), {
        "n_estimators": (50, 300),
        "max_depth": (5, 30),
        "min_samples_split": (2, 10),
        "min_samples_leaf": (1, 5),
        "bootstrap": [True, False]
    }),
    "ExtraTrees": (ExtraTreesRegressor(), {
        "n_estimators": (50, 300),
        "max_depth": (5, 30),
        "min_samples_split": (2, 10),
        "min_samples_leaf": (1, 5),
        "bootstrap": [True, False]
    }),
    "GaussianProcess": (GaussianProcessRegressor(), {
        "alpha": (1e-10, 1e-1, "log-uniform"),
        "n_restarts_optimizer": (0, 10)
    }),
    "KNN": (KNeighborsRegressor(), {
        "n_neighbors": (1, 20),
        "weights": ["uniform", "distance"],
        "p": (1, 2)
    })
}

optimized_params = {}
performance_metrics = {}

for name, (model, param_grid) in models.items():
    print(f"Optimizing {name}...")
    bayes_search = BayesSearchCV(
        estimator=model,
        search_spaces=param_grid,
        n_iter=50,
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        random_state=42
    )
    bayes_search.fit(X_train, y_train)
    optimized_params[name] = bayes_search.best_params_

    best_model = bayes_search.best_estimator_
    y_pred_train = best_model.predict(X_train)
    y_pred_test = best_model.predict(X_test)
    r2_train = r2_score(y_train, y_pred_train)
    r2_test = r2_score(y_test, y_pred_test)
    mae = mean_absolute_error(y_test, y_pred_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

    performance_metrics[name] = {
        "R2 Train": r2_train,
        "R2 Test": r2_test,
        "MAE": mae,
        "RMSE": rmse
    }

    print(f"Best parameters for {name}: {optimized_params[name]}")
    print(f"Performance for {name}: {performance_metrics[name]}\n")

print("\nOptimized Hyperparameters and Metrics for All Models:\n")
for name in models.keys():
    print(f"{name}:")
    print(f"  Best Parameters: {optimized_params[name]}")
    print(f"  Performance: {performance_metrics[name]}")
    print("\n")

"""### **for Am prediction**"""

import numpy as np
import pandas as pd
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from skopt import BayesSearchCV

# Sample dataset (replace with your actual data)
data = pd.read_csv('/content/Processed_RawData_v2d (2).csv')
X = data.iloc[:, 1:7]
ya = data.iloc[:, 18]


# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, ya, test_size=0.2, random_state=42)

# Models and hyperparameter grids
models = {
    "SVR": (SVR(), {
        "C": (1e-3, 1e3, "log-uniform"),
        "epsilon": (1e-3, 1.0, "log-uniform"),
        "gamma": (1e-4, 1.0, "log-uniform"),
        "kernel": ["linear", "rbf"]
    }),
    "RandomForest": (RandomForestRegressor(), {
        "n_estimators": (50, 300),
        "max_depth": (5, 30),
        "min_samples_split": (2, 10),
        "min_samples_leaf": (1, 5),
        "bootstrap": [True, False]
    }),
    "ExtraTrees": (ExtraTreesRegressor(), {
        "n_estimators": (50, 300),
        "max_depth": (5, 30),
        "min_samples_split": (2, 10),
        "min_samples_leaf": (1, 5),
        "bootstrap": [True, False]
    }),
    "GaussianProcess": (GaussianProcessRegressor(), {
        "alpha": (1e-10, 1e-1, "log-uniform"),
        "n_restarts_optimizer": (0, 10)
    }),
    "KNN": (KNeighborsRegressor(), {
        "n_neighbors": (1, 20),
        "weights": ["uniform", "distance"],
        "p": (1, 2)  # Manhattan (1) or Euclidean (2)
    })
}

# Dictionary to store results
optimized_params = {}
performance_metrics = {}

# Tuning and evaluating each model
for name, (model, param_grid) in models.items():
    print(f"Optimizing {name}...")
    bayes_search = BayesSearchCV(
        estimator=model,
        search_spaces=param_grid,
        n_iter=50,
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        random_state=42
    )
    bayes_search.fit(X_train, y_train)

    # Best parameters
    optimized_params[name] = bayes_search.best_params_

    # Predictions
    best_model = bayes_search.best_estimator_
    y_pred_train = best_model.predict(X_train)
    y_pred_test = best_model.predict(X_test)

    # Metrics
    r2_train = r2_score(y_train, y_pred_train)
    r2_test = r2_score(y_test, y_pred_test)
    mae = mean_absolute_error(y_test, y_pred_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

    performance_metrics[name] = {
        "R2 Train": r2_train,
        "R2 Test": r2_test,
        "MAE": mae,
        "RMSE": rmse
    }

    print(f"Best parameters for {name}: {optimized_params[name]}")
    print(f"Performance for {name}: {performance_metrics[name]}\n")

# Print summary of all models
print("\nOptimized Hyperparameters and Metrics for All Models:\n")
for name in models.keys():
    print(f"{name}:")
    print(f"  Best Parameters: {optimized_params[name]}")
    print(f"  Performance: {performance_metrics[name]}")
    print("\n")

"""### **For thermal hysteresis prediction**"""

import numpy as np
import pandas as pd
from sklearn.svm import SVR
from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor
from sklearn.gaussian_process import GaussianProcessRegressor
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from skopt import BayesSearchCV

# Sample dataset (replace with your actual data)
data = pd.read_csv('/content/Processed_RawData_v2d (2).csv')
X = data.iloc[:, 1:7]
yth = data.iloc[:, 16]


# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, yth, test_size=0.2, random_state=42)

# Models and hyperparameter grids
models = {
    "SVR": (SVR(), {
        "C": (1e-3, 1e3, "log-uniform"),
        "epsilon": (1e-3, 1.0, "log-uniform"),
        "gamma": (1e-4, 1.0, "log-uniform"),
        "kernel": ["linear", "rbf"]
    }),
    "RandomForest": (RandomForestRegressor(), {
        "n_estimators": (50, 300),
        "max_depth": (5, 30),
        "min_samples_split": (2, 10),
        "min_samples_leaf": (1, 5),
        "bootstrap": [True, False]
    }),
    "ExtraTrees": (ExtraTreesRegressor(), {
        "n_estimators": (50, 300),
        "max_depth": (5, 30),
        "min_samples_split": (2, 10),
        "min_samples_leaf": (1, 5),
        "bootstrap": [True, False]
    }),
    "GaussianProcess": (GaussianProcessRegressor(), {
        "alpha": (1e-10, 1e-1, "log-uniform"),
        "n_restarts_optimizer": (0, 10)
    }),
    "KNN": (KNeighborsRegressor(), {
        "n_neighbors": (1, 20),
        "weights": ["uniform", "distance"],
        "p": (1, 2)  # Manhattan (1) or Euclidean (2)
    })
}

# Dictionary to store results
optimized_params = {}
performance_metrics = {}

# Tuning and evaluating each model
for name, (model, param_grid) in models.items():
    print(f"Optimizing {name}...")
    bayes_search = BayesSearchCV(
        estimator=model,
        search_spaces=param_grid,
        n_iter=50,
        cv=5,
        scoring='neg_mean_squared_error',
        n_jobs=-1,
        random_state=42
    )
    bayes_search.fit(X_train, y_train)

    # Best parameters
    optimized_params[name] = bayes_search.best_params_

    # Predictions
    best_model = bayes_search.best_estimator_
    y_pred_train = best_model.predict(X_train)
    y_pred_test = best_model.predict(X_test)

    # Metrics
    r2_train = r2_score(y_train, y_pred_train)
    r2_test = r2_score(y_test, y_pred_test)
    mae = mean_absolute_error(y_test, y_pred_test)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred_test))

    performance_metrics[name] = {
        "R2 Train": r2_train,
        "R2 Test": r2_test,
        "MAE": mae,
        "RMSE": rmse
    }

    print(f"Best parameters for {name}: {optimized_params[name]}")
    print(f"Performance for {name}: {performance_metrics[name]}\n")

# Print summary of all models
print("\nOptimized Hyperparameters and Metrics for All Models:\n")
for name in models.keys():
    print(f"{name}:")
    print(f"  Best Parameters: {optimized_params[name]}")
    print(f"  Performance: {performance_metrics[name]}")
    print("\n")